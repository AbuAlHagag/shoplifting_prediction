{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8f1eb8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import os \n",
    "import zipfile\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import h5py\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "import datetime as dt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eb7777af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds\n",
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a23c90bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tf.test.is_built_with_cuda())\n",
    "len(tf.config.experimental.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "24f5317f",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dir = \"Shop DataSet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ac6a50fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "shoplifter_dir = os.path.join(main_dir, 'shop lifters')\n",
    "non_shoplifter_dir = os.path.join(main_dir, 'non shop lifters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "84dedadd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 directories and 0 images in Shop DataSet\n",
      "Found 0 directories and 531 images in Shop DataSet\\non shop lifters\n",
      "Found 0 directories and 324 images in Shop DataSet\\shop lifters\n"
     ]
    }
   ],
   "source": [
    "for dir, dirname, filename in os.walk(main_dir):\n",
    "  print(f\"Found {len(dirname)} directories and {len(filename)} images in {dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "92b2fb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH = 20\n",
    "IMAGE_HEIGHT = 64\n",
    "IMAGE_WIDTH = 64\n",
    "BATCH_SIZE = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "93c0457f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_frames(path, n_frames, frame_step, size):\n",
    "    result = []\n",
    "    src = cv2.VideoCapture(str(path))\n",
    "    \n",
    "    video_length = int(src.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    need_length = 1 + (n_frames - 1) * frame_step\n",
    "    \n",
    "    if need_length > video_length:\n",
    "        start = 0\n",
    "    else:\n",
    "        max_start = video_length - need_length\n",
    "        start = random.randint(0, max_start + 1)\n",
    "        \n",
    "    # Set starting frame\n",
    "    src.set(cv2.CAP_PROP_POS_FRAMES, start)\n",
    "    \n",
    "    # Read first frame\n",
    "    ret, frame = src.read()\n",
    "    if ret:\n",
    "        frame = cv2.resize(frame, size)  # Resize frame to desired size\n",
    "        result.append(format_frame(frame, size))\n",
    "    \n",
    "    # Read subsequent frames\n",
    "    for _ in range(n_frames - 1):\n",
    "        for _ in range(frame_step):\n",
    "            ret, frame = src.read()\n",
    "        if ret:\n",
    "            frame = cv2.resize(frame, size)  # Resize each frame\n",
    "            result.append(format_frame(frame, size))\n",
    "        else:\n",
    "            # If there's an issue with the frame, append a black frame\n",
    "            result.append(np.zeros_like(result[0]))\n",
    "    \n",
    "    src.release()\n",
    "\n",
    "    # Convert to RGB as cv2 reads in BGR\n",
    "    result = np.array(result)[..., [2, 1, 0]]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e2682945",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_frame(frame, size):\n",
    "    \"\"\"Normalize the frame and convert to float16.\"\"\"\n",
    "    frame = tf.convert_to_tensor(frame, dtype=tf.float32) / 255.0  # Normalize to [0, 1]\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f7fc7f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.animation as animation\n",
    "\n",
    "def plot_vid(images):\n",
    "    fig = plt.figure()\n",
    "    frames = []\n",
    "    \n",
    "    for img in images:\n",
    "        frame_plot = plt.imshow(img, animated=True)  # Directly use the image as is\n",
    "        frames.append([frame_plot])\n",
    "\n",
    "    ani = animation.ArtistAnimation(fig, frames, interval=500, blit=True, repeat_delay=1000)\n",
    "    \n",
    "    # Save as MP4 using ffmpeg\n",
    "    ani.save('movie.mp4', writer='ffmpeg')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3f7cedfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "frames = load_frames(\"Shop DataSet\\shop lifters\\shop_lifter_0.mp4\",SEQUENCE_LENGTH,20,size=(IMAGE_HEIGHT,IMAGE_WIDTH))\n",
    "plot_vid(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "93d2e580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 64, 64, 3)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3884c725",
   "metadata": {},
   "source": [
    "## save the picked frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bf1b9e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = []\n",
    "# labels = []\n",
    "# for path in os.listdir(shoplifter_dir):\n",
    "#     frames = load_frames(shoplifter_dir + '/' +path,SEQUENCE_LENGTH,10,size=(IMAGE_HEIGHT,IMAGE_WIDTH))\n",
    "#     data.append(frames)\n",
    "#     labels.append(1)\n",
    "#     print(path + \": done\")\n",
    "# for path in os.listdir(non_shoplifter_dir):\n",
    "#     frames = load_frames(non_shoplifter_dir + '/' + path,SEQUENCE_LENGTH,10,size=(IMAGE_HEIGHT,IMAGE_WIDTH))\n",
    "#     data.append(frames)\n",
    "#     labels.append(0)\n",
    "#     print(path + \": done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "48120a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# # Convert lists to NumPy arrays\n",
    "# data_array = np.array(data)\n",
    "# labels_array = np.array(labels)\n",
    "\n",
    "# # Save to HDF5 file\n",
    "# with h5py.File('dataset.h5', 'w') as hf:\n",
    "#     hf.create_dataset('data', data=data_array)\n",
    "#     hf.create_dataset('labels', data=labels_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7f71cf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from HDF5 file\n",
    "with h5py.File('dataset.h5', 'r') as hf:\n",
    "    data = np.array(hf['data'])\n",
    "    labels = np.array(hf['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "804d2413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (598, 20, 64, 64, 3)\n",
      "Validation data shape: (128, 20, 64, 64, 3)\n",
      "Test data shape: (129, 20, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Convert lists to NumPy arrays if not already done\n",
    "data = np.array(data)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Split data into training (70%) and temp (30%)\n",
    "data_train, data_temp, labels_train, labels_temp = train_test_split(\n",
    "    data, labels, test_size=0.3, random_state=42, stratify=labels)\n",
    "\n",
    "# Split the temp data into validation (50% of temp = 15% of original) and test (50% of temp = 15% of original)\n",
    "data_val, data_test, labels_val, labels_test = train_test_split(\n",
    "    data_temp, labels_temp, test_size=0.5, random_state=42, stratify=labels_temp)\n",
    "\n",
    "# Print shapes to verify\n",
    "print(\"Training data shape:\", data_train.shape)\n",
    "print(\"Validation data shape:\", data_val.shape)\n",
    "print(\"Test data shape:\", data_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4e53c2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metric(model_training_history, metric_name_1, metric_name_2, plot_name):\n",
    "    '''\n",
    "    This function will plot the metrics passed to it in a graph.\n",
    "    Args:\n",
    "        model_training_history: A history object containing a record of training and validation \n",
    "                                loss values and metrics values at successive epochs\n",
    "        metric_name_1:          The name of the first metric that needs to be plotted in the graph.\n",
    "        metric_name_2:          The name of the second metric that needs to be plotted in the graph.\n",
    "        plot_name:              The title of the graph.\n",
    "    '''\n",
    "    \n",
    "    # Get metric values using metric names as identifiers.\n",
    "    metric_value_1 = model_training_history.history[metric_name_1]\n",
    "    metric_value_2 = model_training_history.history[metric_name_2]\n",
    "    \n",
    "    # Construct a range object which will be used as x-axis (horizontal plane) of the graph.\n",
    "    epochs = range(len(metric_value_1))\n",
    "\n",
    "    # Plot the Graph.\n",
    "    plt.plot(epochs, metric_value_1, 'blue', label = metric_name_1)\n",
    "    plt.plot(epochs, metric_value_2, 'red', label = metric_name_2)\n",
    "\n",
    "    # Add title to the plot.\n",
    "    plt.title(str(plot_name))\n",
    "\n",
    "    # Add legend to the plot.\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "da3b8db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: {0: 0.8059299191374663, 1: 1.3171806167400881}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import class_weight\n",
    "import numpy as np\n",
    "\n",
    "# Assuming labels are in a NumPy array (after splitting the dataset)\n",
    "labels_train = np.array(labels_train)  # Ensure labels are NumPy array\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "    class_weight='balanced',  # Set to 'balanced' to adjust for class imbalance\n",
    "    classes=np.unique(labels_train),  # The unique classes (e.g., [0, 1])\n",
    "    y=labels_train  # The labels for training data\n",
    ")\n",
    "\n",
    "# Create a dictionary mapping class to weight\n",
    "class_weight_dict = dict(zip(np.unique(labels_train), class_weights))\n",
    "\n",
    "# Print class weights\n",
    "print(\"Class weights:\", class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8d7c6de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up early stopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',   # Metric to monitor\n",
    "    patience=5,           # Number of epochs with no improvement after which training will be stopped\n",
    "    restore_best_weights=True # Restore model weights from the epoch with the best value of the monitored metric\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9e59988a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReduceLROnPlateau: Reduce the learning rate when the metric has stopped improving.\n",
    "reduce_lr_on_plateau = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',   # Metric to monitor\n",
    "    factor=0.5,           # Factor by which the learning rate will be reduced\n",
    "    patience=3,           # Number of epochs with no improvement to wait before reducing the learning rate\n",
    "    min_lr=1e-6           # Lower bound on the learning rate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ecd11a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ModelCheckpoint: Save the model after every epoch.\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    'best_model.h5',       # Filepath to save the model\n",
    "    monitor='val_loss',     # Metric to monitor\n",
    "    save_best_only=True,    # Only save the best model\n",
    "    save_weights_only=True  # Only save the model's weights\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d887aa82",
   "metadata": {},
   "source": [
    "# models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287bbf0d",
   "metadata": {},
   "source": [
    "## ConvLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ff5e1abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_convlstm_model():\n",
    "    '''\n",
    "    This function will construct the required convlstm model.\n",
    "    Returns:\n",
    "        model: It is the required constructed convlstm model.\n",
    "    '''\n",
    "\n",
    "    # We will use a Sequential model for model construction\n",
    "    model = Sequential()\n",
    "\n",
    "    # Define the Model Architecture.\n",
    "    ########################################################################################################################\n",
    "    \n",
    "    model.add(ConvLSTM2D(filters = 4, kernel_size = (3, 3), activation = 'tanh',data_format = \"channels_last\",\n",
    "                         recurrent_dropout=0.2, return_sequences=True, input_shape = (SEQUENCE_LENGTH,\n",
    "                                                                                      IMAGE_HEIGHT, IMAGE_WIDTH, 3)))\n",
    "    \n",
    "    model.add(MaxPooling3D(pool_size=(1, 2, 2), padding='same', data_format='channels_last'))\n",
    "    model.add(TimeDistributed(Dropout(0.2)))\n",
    "    \n",
    "    model.add(ConvLSTM2D(filters = 8, kernel_size = (3, 3), activation = 'tanh', data_format = \"channels_last\",\n",
    "                         recurrent_dropout=0.2, return_sequences=True))\n",
    "    \n",
    "    model.add(MaxPooling3D(pool_size=(1, 2, 2), padding='same', data_format='channels_last'))\n",
    "    model.add(TimeDistributed(Dropout(0.2)))\n",
    "    \n",
    "    model.add(ConvLSTM2D(filters = 14, kernel_size = (3, 3), activation = 'tanh', data_format = \"channels_last\",\n",
    "                         recurrent_dropout=0.2, return_sequences=True))\n",
    "    \n",
    "    model.add(MaxPooling3D(pool_size=(1, 2, 2), padding='same', data_format='channels_last'))\n",
    "    model.add(TimeDistributed(Dropout(0.2)))\n",
    "    \n",
    "    model.add(ConvLSTM2D(filters = 16, kernel_size = (3, 3), activation = 'tanh', data_format = \"channels_last\",\n",
    "                         recurrent_dropout=0.2, return_sequences=True))\n",
    "    \n",
    "    model.add(MaxPooling3D(pool_size=(1, 2, 2), padding='same', data_format='channels_last'))\n",
    "    #model.add(TimeDistributed(Dropout(0.2)))\n",
    "    \n",
    "    model.add(Flatten()) \n",
    "    \n",
    "    model.add(Dense(1, activation = \"sigmoid\"))\n",
    "    \n",
    "    ########################################################################################################################\n",
    "     \n",
    "    # Compile the model and specify loss function, optimizer and metrics values to the model\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer = 'Adam', metrics = [\"accuracy\"])    \n",
    "        \n",
    "    # Display the models summary.\n",
    "    model.summary()\n",
    "    \n",
    "    # Return the constructed convlstm model.\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3c8337a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv_lstm2d_4 (ConvLSTM2D)  (None, 20, 62, 62, 4)     1024      \n",
      "                                                                 \n",
      " max_pooling3d_4 (MaxPooling  (None, 20, 31, 31, 4)    0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " time_distributed_3 (TimeDis  (None, 20, 31, 31, 4)    0         \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " conv_lstm2d_5 (ConvLSTM2D)  (None, 20, 29, 29, 8)     3488      \n",
      "                                                                 \n",
      " max_pooling3d_5 (MaxPooling  (None, 20, 15, 15, 8)    0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " time_distributed_4 (TimeDis  (None, 20, 15, 15, 8)    0         \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " conv_lstm2d_6 (ConvLSTM2D)  (None, 20, 13, 13, 14)    11144     \n",
      "                                                                 \n",
      " max_pooling3d_6 (MaxPooling  (None, 20, 7, 7, 14)     0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " time_distributed_5 (TimeDis  (None, 20, 7, 7, 14)     0         \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " conv_lstm2d_7 (ConvLSTM2D)  (None, 20, 5, 5, 16)      17344     \n",
      "                                                                 \n",
      " max_pooling3d_7 (MaxPooling  (None, 20, 3, 3, 16)     0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 2880)              0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 1)                 2881      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 35,881\n",
      "Trainable params: 35,881\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model Created Successfully!\n"
     ]
    }
   ],
   "source": [
    "# Construct the required convlstm model.\n",
    "convlstm_model = create_convlstm_model()\n",
    "\n",
    "# Display the success message. \n",
    "print(\"Model Created Successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebd281e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training the model.\n",
    "convlstm_model_training_history = convlstm_model.fit(\n",
    "    x = data_train, \n",
    "    y = labels_train, \n",
    "    epochs = 50, \n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle = True, \n",
    "    validation_data = (data_val, labels_val),  # Use explicit validation data\n",
    "    class_weight = class_weight_dict,\n",
    "    callbacks = [early_stopping, reduce_lr_on_plateau, checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da456c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the trained model.\n",
    "model_evaluation_history = convlstm_model.evaluate(data_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00d7ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric(convlstm_model_training_history, 'loss', 'val_loss', 'Total Loss vs Total Validation Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb63f149",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric(convlstm_model_training_history, 'accuracy', 'val_accuracy', 'Total Accuracy vs Total Validation Accuracy') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f207ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a name\n",
    "model_file_name = f'convlstm_model_0.h5'\n",
    "\n",
    "# Save your Model.\n",
    "convlstm_model.save(model_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b91840a",
   "metadata": {},
   "source": [
    "## Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ab88dcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "88c1f291",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(max_len, d_model):\n",
    "    pos = tf.range(max_len, dtype=tf.float32)[:, tf.newaxis]  # Shape (max_len, 1)\n",
    "    i = tf.range(d_model, dtype=tf.float32)[tf.newaxis, :]    # Shape (1, d_model)\n",
    "    angle_rates = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
    "\n",
    "    pos_encoding = pos * angle_rates  # Shape (max_len, d_model)\n",
    "\n",
    "    # Calculate sine and cosine\n",
    "    pos_encoding_sin = tf.sin(pos_encoding[:, 0::2])  # Sine for even indices\n",
    "    pos_encoding_cos = tf.cos(pos_encoding[:, 1::2])  # Cosine for odd indices\n",
    "\n",
    "    # Combine sine and cosine into one tensor\n",
    "    pos_encoding_final = tf.concat([pos_encoding_sin, pos_encoding_cos], axis=-1)  # Shape (max_len, d_model)\n",
    "\n",
    "    return pos_encoding_final  # Return without adding new axis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f3145393",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.mha = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
    "        self.ffn = models.Sequential([\n",
    "            layers.Dense(dff, activation='relu'),\n",
    "            layers.Dense(d_model)\n",
    "        ])\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training):\n",
    "        attn_output = self.mha(x, x)\n",
    "        out1 = self.layernorm1(x + self.dropout1(attn_output, training=training))\n",
    "        ffn_output = self.ffn(out1)\n",
    "        return self.layernorm2(out1 + self.dropout2(ffn_output, training=training))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d97a634e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_video_transformer(num_classes, input_shape, d_model=128, num_heads=4, dff=512, num_layers=4):\n",
    "    inputs = layers.Input(shape=input_shape)  # Shape: (SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 3)\n",
    "    \n",
    "    # Convolutional Layer to Reduce Dimensions\n",
    "    x = layers.Conv2D(d_model, kernel_size=3, strides=2, padding='same')(inputs)  # Shape: (SEQUENCE_LENGTH, NEW_HEIGHT, NEW_WIDTH, d_model)\n",
    "    \n",
    "    # Reshape to (SEQUENCE_LENGTH, -1, d_model) for Transformer\n",
    "    x = layers.Reshape((input_shape[0], -1, d_model))(x)  # Shape: (SEQUENCE_LENGTH, NEW_HEIGHT * NEW_WIDTH, d_model)\n",
    "\n",
    "    # Calculate Positional Encoding\n",
    "    pos_encoding = positional_encoding(input_shape[0], d_model)  # Shape: (SEQUENCE_LENGTH, d_model)\n",
    "\n",
    "    # Reshape pos_encoding to match x's shape for broadcasting\n",
    "    pos_encoding = tf.expand_dims(pos_encoding, axis=0)  # Shape: (1, SEQUENCE_LENGTH, d_model)\n",
    "    pos_encoding = tf.tile(pos_encoding, [tf.shape(x)[0], 1, 1])  # Now shape is (batch_size, SEQUENCE_LENGTH, d_model)\n",
    "    \n",
    "    # Expand the dimensions of pos_encoding to match the spatial dimensions of x\n",
    "    pos_encoding = tf.expand_dims(pos_encoding, axis=2)  # Shape: (batch_size, SEQUENCE_LENGTH, 1, d_model)\n",
    "    pos_encoding = tf.tile(pos_encoding, [1, 1, tf.shape(x)[2], 1])  # Shape: (batch_size, SEQUENCE_LENGTH, NEW_HEIGHT * NEW_WIDTH, d_model)\n",
    "\n",
    "    x += pos_encoding  # This should now work\n",
    "\n",
    "    # Transformer Layers\n",
    "    for _ in range(num_layers):\n",
    "        x = TransformerEncoder(d_model, num_heads, dff)(x)\n",
    "\n",
    "    # Final Layer\n",
    "    x = layers.GlobalAveragePooling2D()(x)  # Global average pooling\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)  # Output layer\n",
    "\n",
    "    return tf.keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b7e0885e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Graph execution error:\n\nDetected at node 'model_1/transformer_encoder_4/multi_head_attention_4/einsum/Einsum' defined at (most recent call last):\n    File \"D:\\anaconda3\\envs\\tensor\\lib\\runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"D:\\anaconda3\\envs\\tensor\\lib\\runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"D:\\anaconda3\\envs\\tensor\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"D:\\anaconda3\\envs\\tensor\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n      app.start()\n    File \"D:\\anaconda3\\envs\\tensor\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 701, in start\n      self.io_loop.start()\n    File \"D:\\anaconda3\\envs\\tensor\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n      self.asyncio_loop.run_forever()\n    File \"D:\\anaconda3\\envs\\tensor\\lib\\asyncio\\windows_events.py\", line 321, in run_forever\n      super().run_forever()\n    File \"D:\\anaconda3\\envs\\tensor\\lib\\asyncio\\base_events.py\", line 603, in run_forever\n      self._run_once()\n    File \"D:\\anaconda3\\envs\\tensor\\lib\\asyncio\\base_events.py\", line 1909, in _run_once\n      handle._run()\n    File \"D:\\anaconda3\\envs\\tensor\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"D:\\anaconda3\\envs\\tensor\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in dispatch_queue\n      await self.process_one()\n    File \"D:\\anaconda3\\envs\\tensor\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 523, in process_one\n      await dispatch(*args)\n    File \"D:\\anaconda3\\envs\\tensor\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 429, in dispatch_shell\n      await result\n    File \"D:\\anaconda3\\envs\\tensor\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n      reply_content = await reply_content\n    File \"D:\\anaconda3\\envs\\tensor\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 429, in do_execute\n      res = shell.run_cell(\n    File \"D:\\anaconda3\\envs\\tensor\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"D:\\anaconda3\\envs\\tensor\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n      result = self._run_cell(\n    File \"D:\\anaconda3\\envs\\tensor\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n      result = runner(coro)\n    File \"D:\\anaconda3\\envs\\tensor\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"D:\\anaconda3\\envs\\tensor\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"D:\\anaconda3\\envs\\tensor\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"D:\\anaconda3\\envs\\tensor\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\AbuAlHagag\\AppData\\Local\\Temp\\ipykernel_18736\\3589033235.py\", line 12, in <module>\n      video_transformer.fit(data_train,\n    File \"D:\\anaconda3\\envs\\tensor\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"D:\\anaconda3\\envs\\tensor\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"D:\\anaconda3\\envs\\tensor\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"D:\\anaconda3\\envs\\tensor\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"D:\\anaconda3\\envs\\tensor\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"D:\\anaconda3\\envs\\tensor\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n      y_pred = self(x, training=True)\n    File \"D:\\anaconda3\\envs\\tensor\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"D:\\anaconda3\\envs\\tensor\\lib\\site-packages\\keras\\engine\\training.py\", line 557, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"D:\\anaconda3\\envs\\tensor\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"D:\\anaconda3\\envs\\tensor\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"D:\\anaconda3\\envs\\tensor\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"D:\\anaconda3\\envs\\tensor\\lib\\site-packages\\keras\\engine\\functional.py\", line 510, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"D:\\anaconda3\\envs\\tensor\\lib\\site-packages\\keras\\engine\\functional.py\", line 667, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"D:\\anaconda3\\envs\\tensor\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"D:\\anaconda3\\envs\\tensor\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"D:\\anaconda3\\envs\\tensor\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\AbuAlHagag\\AppData\\Local\\Temp\\ipykernel_18736\\3230239993.py\", line 15, in call\n      attn_output = self.mha(x, x)\n    File \"D:\\anaconda3\\envs\\tensor\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"D:\\anaconda3\\envs\\tensor\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"D:\\anaconda3\\envs\\tensor\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"D:\\anaconda3\\envs\\tensor\\lib\\site-packages\\keras\\layers\\attention\\multi_head_attention.py\", line 596, in call\n      attention_output, attention_scores = self._compute_attention(\n    File \"D:\\anaconda3\\envs\\tensor\\lib\\site-packages\\keras\\layers\\attention\\multi_head_attention.py\", line 525, in _compute_attention\n      attention_scores = tf.einsum(self._dot_product_equation, key, query)\nNode: 'model_1/transformer_encoder_4/multi_head_attention_4/einsum/Einsum'\nOOM when allocating tensor with shape[2,4,20480,20480] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node model_1/transformer_encoder_4/multi_head_attention_4/einsum/Einsum}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_15796]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m video_transformer\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Train the model with reduced batch size\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[43mvideo_transformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mlabels_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weight_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce_lr_on_plateau\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\tensor\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\tensor\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node 'model_1/transformer_encoder_4/multi_head_attention_4/einsum/Einsum' defined at (most recent call last):\n    File \"D:\\anaconda3\\envs\\tensor\\lib\\runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"D:\\anaconda3\\envs\\tensor\\lib\\runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"D:\\anaconda3\\envs\\tensor\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"D:\\anaconda3\\envs\\tensor\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n      app.start()\n    File \"D:\\anaconda3\\envs\\tensor\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 701, in start\n      self.io_loop.start()\n    File \"D:\\anaconda3\\envs\\tensor\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n      self.asyncio_loop.run_forever()\n    File \"D:\\anaconda3\\envs\\tensor\\lib\\asyncio\\windows_events.py\", line 321, in run_forever\n      super().run_forever()\n    File \"D:\\anaconda3\\envs\\tensor\\lib\\asyncio\\base_events.py\", line 603, in run_forever\n      self._run_once()\n    File \"D:\\anaconda3\\envs\\tensor\\lib\\asyncio\\base_events.py\", line 1909, in _run_once\n      handle._run()\n    File \"D:\\anaconda3\\envs\\tensor\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"D:\\anaconda3\\envs\\tensor\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in dispatch_queue\n      await self.process_one()\n    File \"D:\\anaconda3\\envs\\tensor\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 523, in process_one\n      await dispatch(*args)\n    File \"D:\\anaconda3\\envs\\tensor\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 429, in dispatch_shell\n      await result\n    File \"D:\\anaconda3\\envs\\tensor\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n      reply_content = await reply_content\n    File \"D:\\anaconda3\\envs\\tensor\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 429, in do_execute\n      res = shell.run_cell(\n    File \"D:\\anaconda3\\envs\\tensor\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"D:\\anaconda3\\envs\\tensor\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n      result = self._run_cell(\n    File \"D:\\anaconda3\\envs\\tensor\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n      result = runner(coro)\n    File \"D:\\anaconda3\\envs\\tensor\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"D:\\anaconda3\\envs\\tensor\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"D:\\anaconda3\\envs\\tensor\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"D:\\anaconda3\\envs\\tensor\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\AbuAlHagag\\AppData\\Local\\Temp\\ipykernel_18736\\3589033235.py\", line 12, in <module>\n      video_transformer.fit(data_train,\n    File \"D:\\anaconda3\\envs\\tensor\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"D:\\anaconda3\\envs\\tensor\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"D:\\anaconda3\\envs\\tensor\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"D:\\anaconda3\\envs\\tensor\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"D:\\anaconda3\\envs\\tensor\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"D:\\anaconda3\\envs\\tensor\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n      y_pred = self(x, training=True)\n    File \"D:\\anaconda3\\envs\\tensor\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"D:\\anaconda3\\envs\\tensor\\lib\\site-packages\\keras\\engine\\training.py\", line 557, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"D:\\anaconda3\\envs\\tensor\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"D:\\anaconda3\\envs\\tensor\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"D:\\anaconda3\\envs\\tensor\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"D:\\anaconda3\\envs\\tensor\\lib\\site-packages\\keras\\engine\\functional.py\", line 510, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"D:\\anaconda3\\envs\\tensor\\lib\\site-packages\\keras\\engine\\functional.py\", line 667, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"D:\\anaconda3\\envs\\tensor\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"D:\\anaconda3\\envs\\tensor\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"D:\\anaconda3\\envs\\tensor\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\AbuAlHagag\\AppData\\Local\\Temp\\ipykernel_18736\\3230239993.py\", line 15, in call\n      attn_output = self.mha(x, x)\n    File \"D:\\anaconda3\\envs\\tensor\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"D:\\anaconda3\\envs\\tensor\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"D:\\anaconda3\\envs\\tensor\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"D:\\anaconda3\\envs\\tensor\\lib\\site-packages\\keras\\layers\\attention\\multi_head_attention.py\", line 596, in call\n      attention_output, attention_scores = self._compute_attention(\n    File \"D:\\anaconda3\\envs\\tensor\\lib\\site-packages\\keras\\layers\\attention\\multi_head_attention.py\", line 525, in _compute_attention\n      attention_scores = tf.einsum(self._dot_product_equation, key, query)\nNode: 'model_1/transformer_encoder_4/multi_head_attention_4/einsum/Einsum'\nOOM when allocating tensor with shape[2,4,20480,20480] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node model_1/transformer_encoder_4/multi_head_attention_4/einsum/Einsum}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_15796]"
     ]
    }
   ],
   "source": [
    "# Define parameters\n",
    "num_classes = 2  # Adjust based on your dataset\n",
    "input_shape = (SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 3)  # Change based on your data\n",
    "\n",
    "# Create the model\n",
    "video_transformer = create_video_transformer(num_classes, input_shape)\n",
    "\n",
    "# Compile and train the model\n",
    "video_transformer.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model with reduced batch size\n",
    "video_transformer.fit(data_train,\n",
    "                      labels_train,\n",
    "                      epochs=50,\n",
    "                      batch_size=BATCH_SIZE,\n",
    "                      validation_data=(data_val, labels_val),\n",
    "                      class_weight=class_weight_dict,\n",
    "                      callbacks=[early_stopping, reduce_lr_on_plateau, checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a80b928",
   "metadata": {},
   "source": [
    "# more models examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66889783",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_LRCN_model():\n",
    "    '''\n",
    "    This function will construct the required LRCN model.\n",
    "    Returns:\n",
    "        model: It is the required constructed LRCN model.\n",
    "    '''\n",
    "\n",
    "    # We will use a Sequential model for model construction.\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Define the Model Architecture.\n",
    "    ########################################################################################################################\n",
    "    \n",
    "    model.add(TimeDistributed(Conv2D(16, (3, 3), padding='same',activation = 'relu'),\n",
    "                              input_shape = (SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 3)))\n",
    "    \n",
    "    model.add(TimeDistributed(MaxPooling2D((4, 4)))) \n",
    "    model.add(TimeDistributed(Dropout(0.25)))\n",
    "    \n",
    "    model.add(TimeDistributed(Conv2D(32, (3, 3), padding='same',activation = 'relu')))\n",
    "    model.add(TimeDistributed(MaxPooling2D((4, 4))))\n",
    "    model.add(TimeDistributed(Dropout(0.25)))\n",
    "    \n",
    "    model.add(TimeDistributed(Conv2D(64, (3, 3), padding='same',activation = 'relu')))\n",
    "    model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "    model.add(TimeDistributed(Dropout(0.25)))\n",
    "    \n",
    "    model.add(TimeDistributed(Conv2D(64, (3, 3), padding='same',activation = 'relu')))\n",
    "    model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "    model.add(TimeDistributed(Dropout(0.25)))\n",
    "                                      \n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "                                      \n",
    "    model.add(LSTM(128))\n",
    "                                      \n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))    \n",
    "    model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "    ########################################################################################################################\n",
    "\n",
    "    # Display the models summary.\n",
    "    model.summary()\n",
    "    \n",
    "    # Return the constructed LRCN model.\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52448b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_resnet_video_model(input_shape, sequence_length):\n",
    "    base_model = tf.keras.applications.ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(TimeDistributed(base_model, input_shape=(sequence_length,) + input_shape))\n",
    "\n",
    "    model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "\n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "    \n",
    "    model.add(LSTM(512, return_sequences=False))\n",
    "    \n",
    "    \n",
    "    \n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c267bf48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f44ca34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e357c17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82376db0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60696d2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a10be4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
